{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e8698e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "632abb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EOD Data Load\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfe2d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.1.1: Read Trade Partition Dataset From Its Temporary Location\n",
    "#trade_common = spark.read.parquet(\"output_dir/partition=T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "781de1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+------+--------+-------------------+------------+-------------------+--------+-------------------+---------+\n",
      "|  trade_dt|rec_type|symbol|exchange|           event_tm|event_seq_nb|         arrival_tm|trade_pr|            file_tm|partition|\n",
      "+----------+--------+------+--------+-------------------+------------+-------------------+--------+-------------------+---------+\n",
      "|2020-07-29|       T|  AAPL|  NASDAQ|2020-07-29 09:30:00|           1|2020-07-29 09:30:05|   150.0|2020-07-29 09:30:10|        T|\n",
      "|2020-07-29|       T|  MSFT|  NASDAQ|2020-07-29 09:31:00|           2|2020-07-29 09:31:05|   230.0|2020-07-29 09:31:10|        T|\n",
      "|2020-07-29|       T|  GOOG|  NASDAQ|2020-07-29 09:32:00|           3|2020-07-29 09:32:05|  2750.0|2020-07-29 09:32:10|        T|\n",
      "+----------+--------+------+--------+-------------------+------------+-------------------+--------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Read the CSV file\n",
    "csv_file_path = r\"C:\\Users\\march\\Dropbox\\DE Springboard\\Guided Capstone\\output_dir\\partition=T\\sample_trade_data_with_tm.csv\"  # Adjust the path accordingly\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show the DataFrame to verify it's read correctly\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01a634db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+------+--------+-------------------+------------+-------------------+--------+-------------------+---------+\n",
      "|  trade_dt|rec_type|symbol|exchange|           event_tm|event_seq_nb|         arrival_tm|trade_pr|            file_tm|partition|\n",
      "+----------+--------+------+--------+-------------------+------------+-------------------+--------+-------------------+---------+\n",
      "|2020-07-29|       T|  AAPL|  NASDAQ|2020-07-29 09:30:00|           1|2020-07-29 09:30:05|   150.0|2020-07-29 09:30:10|        T|\n",
      "|2020-07-29|       T|  MSFT|  NASDAQ|2020-07-29 09:31:00|           2|2020-07-29 09:31:05|   230.0|2020-07-29 09:31:10|        T|\n",
      "|2020-07-29|       T|  GOOG|  NASDAQ|2020-07-29 09:32:00|           3|2020-07-29 09:32:05|  2750.0|2020-07-29 09:32:10|        T|\n",
      "+----------+--------+------+--------+-------------------+------------+-------------------+--------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Write the DataFrame to Parquet format\n",
    "output_parquet_path = r\"C:\\Users\\march\\Downloads\\trades.parquet\"  # Adjust the path accordingly\n",
    "df.write.mode(\"overwrite\").parquet(output_parquet_path)\n",
    "\n",
    "# Step 3: Read the Parquet file\n",
    "trade_common = spark.read.parquet(output_parquet_path)\n",
    "\n",
    "# Show the DataFrame from the Parquet file to verify\n",
    "trade_common.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2252e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trade_dt: date (nullable = true)\n",
      " |-- symbol: string (nullable = true)\n",
      " |-- exchange: string (nullable = true)\n",
      " |-- event_tm: timestamp (nullable = true)\n",
      " |-- event_seq_nb: integer (nullable = true)\n",
      " |-- file_tm: timestamp (nullable = true)\n",
      " |-- trade_pr: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3.1.2: Select Necessary Columns for Trade Records\n",
    "trade = trade_common.select(\n",
    "    \"trade_dt\", \n",
    "    \"symbol\", \n",
    "    \"exchange\", \n",
    "    \"event_tm\",\n",
    "    \"event_seq_nb\", \n",
    "    \"file_tm\",\n",
    "    \"trade_pr\"\n",
    ")\n",
    "\n",
    "# Display the schema of the selected DataFrame\n",
    "trade.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d21865e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------+-------------------+------------+-------------------+--------+\n",
      "|  trade_dt|symbol|exchange|           event_tm|event_seq_nb|            file_tm|trade_pr|\n",
      "+----------+------+--------+-------------------+------------+-------------------+--------+\n",
      "|2020-07-29|  GOOG|  NASDAQ|2020-07-29 09:32:00|           3|2020-07-29 09:32:10|  2750.0|\n",
      "|2020-07-29|  MSFT|  NASDAQ|2020-07-29 09:31:00|           2|2020-07-29 09:31:10|   230.0|\n",
      "|2020-07-29|  AAPL|  NASDAQ|2020-07-29 09:30:00|           1|2020-07-29 09:30:10|   150.0|\n",
      "+----------+------+--------+-------------------+------------+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3.1.3: Apply Data Correction\n",
    "# Define the function to apply the latest records based on the arrival time\n",
    "def applyLatest(df):\n",
    "    # Group by the unique ID and get the latest arrival time\n",
    "    return df.groupBy(\"trade_dt\", \"symbol\", \"exchange\", \"event_tm\", \"event_seq_nb\") \\\n",
    "        .agg(F.last(\"file_tm\").alias(\"latest_file_tm\"), \n",
    "        F.last(\"trade_pr\").alias(\"trade_pr\")) \\\n",
    "        .withColumnRenamed(\"latest_file_tm\", \"file_tm\")\n",
    "\n",
    "# Apply the function to the trade DataFrame\n",
    "trade_corrected = applyLatest(trade)\n",
    "\n",
    "# Show the corrected DataFrame\n",
    "trade_corrected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a074062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.1.4: Write The Trade Dataset Back To Parquet On Azure Blob Storage\n",
    "trade_date = \"2020-07-29\"  # Example date for partitioning\n",
    "\n",
    "cloud_storage_path =  r\"C:\\Users\\march\\Downloads\\trade_dt={}\".format(trade_date) \n",
    "#cloud-storage-path/trade/trade_dt={}\".format(trade_date)\n",
    "\n",
    "# Write the corrected trade DataFrame back to Azure Blob Storage\n",
    "trade_corrected.write.parquet(cloud_storage_path)\n",
    "\n",
    "# Summary\n",
    "# In this notebook, we have practiced data normalization and using cloud storage with Spark output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd126a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to run SQL queries against trade and quote data on Azure, how would you\n",
    "# do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e41aeed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------+-------------------+------------+-------------------+--------+\n",
      "|  trade_dt|symbol|exchange|           event_tm|event_seq_nb|            file_tm|trade_pr|\n",
      "+----------+------+--------+-------------------+------------+-------------------+--------+\n",
      "|2020-07-29|  GOOG|  NASDAQ|2020-07-29 09:32:00|           3|2020-07-29 09:32:10|  2750.0|\n",
      "|2020-07-29|  MSFT|  NASDAQ|2020-07-29 09:31:00|           2|2020-07-29 09:31:10|   230.0|\n",
      "+----------+------+--------+-------------------+------------+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Use Spark SQL to run queries on the temporary view\n",
    "# Example query: Get all trades --where trade price is greater than 200\n",
    "\n",
    "# Step 2: Create a temporary view\n",
    "trade_corrected.createOrReplaceTempView(\"trades_view\")\n",
    "\n",
    "query_result = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM trades_view \n",
    "\"\"\")\n",
    "#WHERE trade_pr > 200\n",
    "\n",
    "# Show the results of the query\n",
    "query_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34572468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Azure Data Bricks\n",
    "\n",
    "%sql\n",
    "SELECT *\n",
    "FROM trade_table #change table name\n",
    "WHERE exchange = 'NASDAQ';"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
